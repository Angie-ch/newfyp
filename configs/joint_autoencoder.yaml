# Configuration for Joint Autoencoder Training
# Encodes ERA5 + IBTrACS together, decodes separately

data:
  data_dir: "data/processed_temporal_split"
  past_frames: 8
  future_frames: 12
  era5_channels: 48

model:
  era5_channels: 48
  latent_channels: 8
  hidden_dims: [64, 128, 256, 256]
  use_attention: true

training:
  epochs: 50
  batch_size: 8
  learning_rate: 1.0e-4
  weight_decay: 0.01
  gradient_clip: 1.0
  
  # Loss weights
  era5_weight: 1.0
  track_weight: 10.0      # Higher weight for track accuracy
  intensity_weight: 5.0   # Higher weight for intensity accuracy
  
  # Logging
  log_interval: 100
  save_interval: 5
  log_dir: "logs/joint_autoencoder"
  save_dir: "checkpoints/joint_autoencoder"
  
  # Data loading
  num_workers: 0  # Set to 0 to avoid multiprocessing issues

