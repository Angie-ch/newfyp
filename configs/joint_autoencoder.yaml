# Configuration for Joint Autoencoder Training
# Encodes ERA5 + IBTrACS together, decodes separately

data:
  data_dir: "data/processed_temporal_split"
  past_frames: 8
  future_frames: 12
  era5_channels: 48

model:
  era5_channels: 48
  latent_channels: 8
  hidden_dims: [64, 128, 256, 256]
  use_attention: true
  dropout: 0.1  # Dropout for regularization

training:
  epochs: 50  # Train for 50 epochs
  batch_size: 8
  learning_rate: 1.0e-4
  weight_decay: 0.05  # Increased from 0.01 for better regularization
  gradient_clip: 1.0
  
  # Loss weights - adjusted for normalized data
  era5_weight: 1.0
  track_weight: 1.0      # Reduced since data is now normalized
  intensity_weight: 1.0  # Reduced since data is now normalized
  
  # Early stopping
  early_stopping_patience: 5
  
  # Logging
  log_interval: 100
  save_interval: 5
  log_dir: "logs/joint_autoencoder"
  save_dir: "checkpoints/joint_autoencoder"
  
  # Data loading
  num_workers: 0  # Set to 0 to avoid multiprocessing issues

